<!DOCTYPE html>
<html lang="en" class="h-100">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel = "icon" href = "images/bu-logo.png" 
        type = "image/x-icon">
    <title>shantanu-ai</title>
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,700&display=swap" rel="stylesheet">
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
            integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
            crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
            integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
            crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
            integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
            crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css"
          integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">


    <style>
/*        @import url('https://rsms.me/inter/inter.css');*/
        body, h1, h2, h3, h4, h5, h6, p, ul, li, nav, div, a, button, input, textarea {
            font-family: "Open Sans", "Roboto", Arial, sans-serif !important;
        }

        /* Ensure all elements have the font-weight you prefer */
        /* For example, to make the text normal weight: */
        body, p, ul, li, nav, div, a, button, input, textarea {
            font-weight: 400 !important;
            font-size: 15px !important;
        }

        /* For headings to have bold weight: */
        h1, h2, h3, h4, h5, h6 {
            font-weight: 700 !important;
        }
        .video-container {
            position: relative;
            width: 100%;
            padding-bottom: 56.25%; /* Aspect ratio of 16:9 */
            height: 0;
            overflow: hidden;
            margin-bottom: 20px; /* Spacing between tiles */
        }
        .video-title {
            text-align: center; /* Center the title if desired */
            /* Any additional styling you want for the title */
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        #navbarNav a {
            color: rgba(255, 255, 255, .7);
            font-weight: 600;
        }

        .headshot {
            max-width: 300px;
            border: 2px solid gray;
            border-radius: 0;
            background: none;
        }

        #date { 
            color: #888888;
        }

        p, div {
            text-align: justify;
        }
        html,
        * {
            font-family: 'Inter', sans-serif !important;

            letter-spacing: -0.011em;
            /*font-family: 'Jost', sans-serif !important;*/
            /*font-size: 18px;*/
        }

        @supports (font-variation-settings: normal) {

            html,
            * {
                font-family: 'Inter var', sans-serif !important;
            }
        }

        h4 {
            font-weight: bold;
            letter-spacing: -0.019em;
        }

        p, ul, header {
            /*font-size: 1.2rem !important;*/
            /*letter-spacing: 0.01em;*/
        }

        nav *, h4 {
            /*letter-spacing: 0.05em;*/
        }

        .placeholder {
            margin-right: 1rem !important;
            width: 111px;
            height: 111px;
            border-radius: 0.25rem;
            border: none;
        }

        nav.navbar {
            margin: 0 auto;
            /* center */
        }

        .class-listing {
            display: flex;
            justify-content: space-between;
        }

        .class-listing span {
            margin: 5px 0;
        }

        a.disabledlink {
            /* Make the disabled links grayish*/
            color: gray;
            /* And disable the pointer events */
            pointer-events: none;
        }

        .paperitem {
            display: flex;
            align-items: center;
            margin-top: 1rem !important;
            margin-bottom: 1rem !important;
        }

        .paperitem video, .paperitem img {
            margin-right: 1rem !important;
            width: 205px;
            /*border-radius: 0.25rem;*/
            border: 1px solid #6c757d !important;
        }

        .paperitem span.new {
            margin-left: 7px;
            padding: 3px 5px;
            color: white;
            /* font-weight: 700; */
            /* font-weight: 900; */
            text-transform: uppercase;
            /* letter-spacing: -0.05em; */
            border-radius: 3px;
            background-color: #F44336;
            align-self: center;
            font-size: 80%;
        }

        .paperitem > div {
            color: #6c757d !important;
            /*font-size: 1.25em;*/
        }

        .paperitem > div > b:first-child {
            color: #212529;
            display: flex;
        }

        .bg-secondary{
            background-color: #2b3969!important;
        }

        #navbarNav a {
            font-weight: 600;
        }

        @media (max-width: 1000px) {
            /*p, ul, header {
                font-size: 1.1rem !important;
            }*/
            .class-listing {
                flex-direction: column;
                margin-bottom: 10px;
            }

            .class-listing span {
                margin: 0;
            }
        }

        .child {
            position: relative;
            display: block;
            width: 100%;
            padding: 0;
            overflow: hidden;
        }

        @media (min-width: 1000px)
        .col-md-4 {
            -webkit-box-flex: 0;
            -ms-flex: 0 0 49%;
            flex: 0 0 49%;
            max-width: 49%;
        }
        div .text {
            text-align: center;
            display: block; /* Optional: Ensure the span takes the full width */
        }

    </style>
</head>
<body class="h-100 d-flex flex-column">
<header class="bg-light fixed-top" style="background-color: #2b3969!important;">
    <nav class="navbar container navbar-expand-sm navbar-light">
        <span style="font-weight: 500; letter-spacing: -0.017em;">
            <a class="navbar-brand" href="#" style="color: #fff;font-size: 19px !important;font-weight: 550 !important;">Shantanu Ghosh (he/him)</a>
        </span>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav" style="margin-left: auto">
                <li class="nav-item">
                    <a class="nav-link" href="#bio">About</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#research">Research</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#publication">Publications</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#teaching">Teaching</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#contact">Contact</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="files/Simple_Cover_Shantanu.pdf" class="text-light">Cover Letter</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="files/Shantanu_CV_PhD.pdf" class="text-light">CV</a>
                </li>
            </ul>
        </div>
    </nav>
</header>
<main class="container flex-grow-1" style="padding-top:60px">
    <div class="row my-4" id="bio">
        <div class="col-md-auto">
            <img class="headshot" src="images/Shantanu-zion.jpg"/>
            <div style="max-width: 300px; text-align: left">
                <div style="padding-top: 2%; text-align: left">
                    <i>I investigate the intelligence encoded inside a deep neural network. </i>
                    <br/>
                    <a href="https://www.bu.edu/eng/academics/departments-and-divisions/electrical-and-computer-engineering/">Department
                        of Electrical and Computer Engineering</a>, Boston University
                    <br/> 407-07, Photonics Center, 8 St Mary's St, Boston, MA 02215
                    <br/> shawn24 [at] bu [dot] edu
                </div>
                <!--                <div>Boston University</div>-->
                <!--                <div style="padding-top: 5%"></div>-->
                <!--                <div style="padding-top: 5%">shawn24 [at] bu [dot] edu</div>-->
            </div>
        </div>
        <div class="col-md mt-4 mt-md-0" style="align-self:center; margin-top: -1.5% !important">
            <p>
                I am a lifelong proud <a href="https://en.wikipedia.org/wiki/Albert_and_Alberta_Gator">gator</a> <img
                    src="images/Florida_Gators_gator_logo.svg.png" style="
    height: 0%;width: 4%;"> and a PhD candidate in Electrical Engineering at <a
                    href="https://www.bu.edu/eng/academics/departments-and-divisions/electrical-and-computer-engineering/">
                Boston University</a>, advised by Prof. <a
                    href="https://www.batman-lab.com/kayhan-batmanghelich-biography/">Kayhan Batmanghelich</a> at <a href="https://www.batman-lab.com/">Batman Lab</a>. I collaborate closely with<a href="https://www.bumc.bu.edu/camed/profile/clare-poynton/"> Dr. Clare B. Poynton</a> from Boston University Medical Campus. Before our lab moved to
                Boston, I was a PhD student in the Intelligent Systems Program (ISP) at the <a
                    href="https://www.isp.pitt.edu/">University of Pittsburgh</a>. While at Pitt, I used to collaborate with  <a
                    href="https://forougha.github.io/">Dr. Forough Arabshahi</a> from Meta, Inc. At Pitt, I was also a cross-registered student at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>,
                where I registered for the courses Foundations of Causation and Machine Learning (PHI 80625)
                and <a href="https://visual-learning.cs.cmu.edu/">Visual Learning and Recognition (RI
                16-824)</a>. My current research interest lies in explainable AI by leverageing vision language representations to understand, explain and audit any pre-trained deep neural network. I believe that understanding a deep model's behavior is essential to mitigating bias and engendering trust in AI.
            </p>
            <p>
                Prior to that, I graduated with a Master's degree in <a href="https://www.cise.ufl.edu/">Computer Science</a> from the <a href="https://www.ufl.edu/">University of Florida</a>. I was fortunate to work
                as a graduate assistant in <a
                    href="https://epidemiology.phhp.ufl.edu/research/disl/">Data Intelligence Systems Lab (DISL)</a> lab
                under the
                supervision of Prof. <a href="https://epidemiology.phhp.ufl.edu/profile/prosperi-mattia/">
                Mattia Prosperi</a> and <a href="http://jiangbian.me/"> Prof. Jiang Bian</a>, where I conducted
                research on the intersection of deep learning and causal inference. I also worked closely
                with <a href="https://www.cise.ufl.edu/~butler/">Prof. Kevin Butler</a> as a Graduate
                Research Assistant at the <a href="https://fics.institute.ufl.edu/">Florida Institute of
                Cybersecurity (FICS)</a> Research.
            </p>

            <p style="text-align:left">
                [
                <a href="https://scholar.google.com/citations?user=U_s5k_oAAAAJ&hl=en">Google Scholar</a>
                &nbsp|&nbsp
                <a href="https://www.semanticscholar.org/author/Shantanu-Ghosh/152709682">Semantic Scholar</a>
                &nbsp|&nbsp
                <a href="https://openreview.net/profile?id=~Shantanu_Ghosh2">OpenReview</a>
                &nbsp|&nbsp
                <a href="https://github.com/shantanu-ai">Github</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/shantanuai/">LinkedIn</a> &nbsp|&nbsp
                <a href="https://twitter.com/shantanuai">Twitter</a>
                ]
            </p>

        </div>
    </div>

    <div class="row my-4" id="universities">
        <div class="col-md-auto">
            <table width="100%" align="center" border="0" cellpadding="10">
                <tbody>
                <tr>
                    <!-- <heading>Affiliations</heading> -->
                    <td align="center" width="16%"
                        style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                        <a href="https://www.bu.edu/eng/academics/departments-and-divisions/electrical-and-computer-engineering/"><img
                                src="images/Boston_University_Terriers_logo.svg.png" style="width: 40%"></a>
                    </td>
                    <td align="center" width="16%"
                        style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                        <a href="https://www.isp.pitt.edu/"><img src="images/Pittsvg.png" style="width: 40%"></a>
                    </td>

                    <td align="center" width="16%"
                        style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                        <a href="https://www.cmu.edu/"><img src="images/cmu_logo.png" style="width: 40%"></a>
                    </td>

                    <td align="center" width="16%"
                        style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
                        <a href="https://www.ufl.edu/"><img src="images/UF_logo.png" style="width: 50%"></a>
                    </td>
                </tr>
                </tbody>
            </table>

        </div>
    </div>

    <div class="row my-4" id="research">
        <div class="col-md-auto">
            <h4>Research</h4>
            <p>
                My research interests span computer vision and medical image analysis, focusing on interpretability and explainable AI (X-AI). My work involves investigating different architectures of supervised, unsupervised and self-supervised deep neural networks to improve their generalizability, robustness, and trustworthiness. Specifically, I aim to answer the following research questions:
                <br/>
                1. Can we decipher the failure modes of deep neural networks through multimodal vision-language representations and large language models (LLMs) for improved reliability and debugging? (<span style="color:red;"><b>Ongoing</b></span>)<br/>
                2. Can we develop new vision-language foundation models for previously unexplored imaging modalities, e.g, breast mammograms in biomedical AI? (<span style="color:red;"><b>Ongoing</b></span>)<br/>
                3. Can we extract interpretable models out of a pre-trained deep neural network, often treated as a black box, with the help of high-level human interpretable concepts? [<b>MoIE</b> (<a href="https://proceedings.mlr.press/v202/ghosh23c/ghosh23c.pdf">ICML 2023</a> + <a href="https://openreview.net/pdf?id=m5vnLHfNy7">SCIS@ICML 2023</a>)]<br/>
                4. Can we use the concept-based interpretable models for better data and computational efficiency? [<b>MoIE-CXR</b> (<a href="https://link.springer.com/chapter/10.1007/978-3-031-43895-0_59">MICCAI 2023</a> + <a href="https://openreview.net/pdf?id=sbOVNiSNY8">IMLH@ICML 2023</a>)]<br/> 
                5. Can we leverage weak labels from the radiology reports to localize a disease and its progression without relying on ground-truth bounding box annotation? [<b>AGXNet</b> (<a href="https://link.springer.com/chapter/10.1007/978-3-031-16443-9_63">MICCAI 2022</a>)]<br/>
                <!-- My current research is summarised by my advisor Dr. Kayhan Batmanghelich, in his recent talk at the prestigious <a href="http://computationalgenomics.bioinformatics.ucla.edu/">Computational Genomics Summer Institute CGSI, 2023</a> at the University of California, Los Angeles (UCLA).<br/>
                <div class="video-container" style="margin-bottom: -5%;">
                    <iframe align="center" width="550" height="350" src="https://www.youtube.com/embed/zfE6zMowBnY?si=sZrAPhBGz8atNnWf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div> -->
                <!-- <div class="video-container">
                    <iframe align="center" width="550" height="350" src="https://www.youtube.com/embed/zfE6zMowBnY?si=sZrAPhBGz8atNnWf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div> -->
            </p>
            <p> During my time at UF, I was interested in causal inference and developed
                deep learning algorithms [ <a href="https://academic.oup.com/jamia/article-abstract/28/6/1197/6139936?redirectedFrom=fulltext">DPN-SA (JAMIA 2021)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S2666990021000197?via%3Dihub">PSSAM-GAN (CMPB-U 2021) </a>and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10148269/">DR-VIDAL (AMIA 2022)</a> ] to calculate propensity
                scores for the efficient estimation of individual treatment effects (ITE). For a detailed
                overview of my Master's research, refer to the slides available at this <a
                    href="https://docs.google.com/presentation/d/1J5rPYsREoZhIQmybDxGy0Q-JVmDu76Kssfi08sMekiY/edit#slide=id.gfa4b813cc6_0_436">link</a>.
            </p>
        </div>
    </div>

    <div class="row my-4" id="news">
        <div class="col-md-auto">
            <h4>News</h4>
            <table>
                <tbody>
                <tr>
                    <td>
                        <span id="date">[Oct 2023]</span> I'm serving as a reviewer for ICLR 2024, Medical Image Analysis (MedIA) and CVPR 2024. <br/>
                        <span id="date">[Aug 2023]</span> I'm invited to serve as a Program Committee (PC) member for AAAI 2024. <br>
                        <span id="date">[Jul 2023]</span> I'm serving as a reviewer for NeurIPS 2023 and the
                        journal Computer Methods and Programs in Biomedicine.<br>
                        <span id="date">[Jun 2023]</span> I'm now a PhD candidate. Also, two papers are accepted at <strong><a
                            href="https://sites.google.com/view/scis-workshop-23">SCIS </a></strong> and <strong><a
                            href="https://sites.google.com/view/imlh2023/home?authuser=1">IMLH</a></strong> workshops at
                        <a href="https://icml.cc/Conferences/2023"><strong> ICML
                            2023</strong></a>.<br>
                        <span id="date">[May 2023]</span> Our work <a
                            href="https://shantanu-ai.github.io/projects/MICCAI-2023-MoIE-CXR/"><strong>Distilling
                        BlackBox to Interpretable models for
                        Efficient Transfer Learning</strong></a> is accepted (<span style="color:#C82506;"><strong>Early accept, top ~ 14%</strong></span>)
                        at <a
                            href="https://conferences.miccai.org/2023/en/"><strong> MICCAI
                        2023</strong></a>.<br>
                        <span id="date">[Apr 2023]</span> Our work <a
                            href="https://shantanu-ai.github.io/projects/ICML-2023-MoIE/"><strong>Dividing
                        and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret,
                        Repeat</strong></a> is accepted at <a href="https://icml.cc/Conferences/2023"><strong> ICML
                        2023</strong></a>.<br>
                        <span id="date">[Dec 2022]</span> I'm joining <a
                            href="https://www.bu.edu/eng/academics/departments-and-divisions/electrical-and-computer-engineering/">Boston
                        University</a> in Spring 2023 in the Department of Electrical and Computer Engineering
                        following my advisor's move. My research will be supported by
                        Doctoral Research Fellowship.<br>
                        <span id="date">[Jun 2022]</span> Our work on doubly robust estimation of ITE is accepted as an
                        <strong><span style="color:#C82506;">oral presentation</span></strong> at the <a
                            href="https://amia.org/education-events/amia-2022-annual-symposium/calls-participation/">AMIA
                        2022 Annual Sympossium</a>.<br>
                        <span id="date">[Jun 2022]</span> Our work on weakly supervised disease localization is accepted at
                        <a href="http://www.miccai.org/">MICCAI 2022</a>.<br>
                        <span id="date">[Aug 2021]</span> I'm joining the <a href="https://www.isp.pitt.edu/">University
                        of Pittsburgh</a> in the Intelligent Systems Program under the supervision of Dr. <a
                            href="https://www.batman-lab.com/">Kayhan Batmanghelich</a> in Fall 2021.<br>
                        <span id="date">[May 2021]</span> I graduated with a Master's degree in <a
                            href="https://www.cise.ufl.edu/">Computer Science</a> from the <a
                            href="https://www.ufl.edu/">University of Florida </a>. <em><span
                            style="color:#C82506;"><strong>Go Gators!!</strong> <img
                            src="images/Florida_Gators_gator_logo.svg.png"
                            style="height: 0%;width: 3%;"></span></em><br>
                        <span id="date">[Apr 2021]</span> Our work to balance the unmatched controlled samples by
                        simulating
                        treated samples using GAN, is accepted in the <a
                            href="https://www.sciencedirect.com/science/article/pii/S2666990021000197/"> Journal of
                        Computer methods and programs in biomedicine update</a>.<br>
                        <span id="date">[Dec 2020]</span> Our work to estimate the Propensity score by dimensionality
                        reduction using an autoencoder, is accepted in the <a
                            href="https://pubmed.ncbi.nlm.nih.gov/33594415/"> Journal of the American Medical
                        Informatics Association</a>.<br>
                        <span id="date">[Apr 2020]</span> I'm joining <a
                            href="https://epidemiology.phhp.ufl.edu/research/disl/">DISL</a> lab as a graduate assistant
                        under the
                        supervision of Prof.<a href="https://epidemiology.phhp.ufl.edu/profile/prosperi-mattia/">
                        Mattia Prosperi</a> and <a href="http://jiangbian.me/"> Prof. Jiang Bian</a>.<br>
                        <span id="date">[Aug 2019]</span> I'm moving to the US to join the Master's program in the department of <a
                            href="https://www.cise.ufl.edu/">Computer Science</a>
                        in the <a href="https://www.ufl.edu/">University of Florida</a> in Fall 2019.<br>
                    </td>
                </tr>

                </tbody>
            </table>
        </div>
    </div>

    <div class="row my-4" id="publication">
        <div class="col">
            <h4>Publications</h4>
            <div>
                <ul style='list-style-type:none;margin-bottom: 0!important;padding-left: 0!important'>
                    <li class="paperitem">
                        <img alt="" src="images/miccai-2023.png" class="publogo float-left" width="100">
                        <div>
                            <b>Distilling BlackBox to Interpretable models for Efficient Transfer Learning</b>
                            <b>Shantanu Ghosh</b>,
                            <a href="https://gatechke.github.io/" style="color: rgb(136, 136, 136);">Ke Yu</a>,
                            <a href="https://www.batman-lab.com/" style="color: rgb(136, 136, 136);">
                                Kayhan Batmanghelich
                            </a>
                            <br/>
                            <a href="https://shantanu-ai.github.io/projects/MICCAI-2023-MoIE-CXR/" class="mr-3">
                                Project Page
                            </a>
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-43895-0_59" class="mr-3">Paper</a>
                            <a href="https://arxiv.org/pdf/2305.17303.pdf" class="mr-3">arXiv</a>
                            <a href="https://openreview.net/forum?id=sbOVNiSNY8"
                               class="mr-3">
                                Workshop Paper
                            </a>
                            <a href="https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs" class="mr-3">
                                Code
                            </a>
                            <a href="https://shantanu-ai.github.io/projects/MICCAI-2023-MoIE-CXR/static/data/Route-interpret-repeat-transfer-learning-miccai-23-v3-long.pdf"
                               class="mr-3">Slides</a>
                            <a href="https://shantanu-ai.github.io/projects/MICCAI-2023-MoIE-CXR/static/data/Route-Interpret-Repeat-transfer-learning-miccai-23-poster-v1.pdf"
                               class="mr-3">Poster</a>
                            <a href="https://www.youtube.com/watch?v=tG_Zr_8ton0" class="mr-3">Video</a>
                            <a href="https://conferences.miccai.org/2023/papers/212-Paper2523.html" class="mr-3">Reviews</a>
                            <br/>
                            <span style="color:black;">26<sup>th</sup> International Conference on Medical Image Computing and Computer Assisted
                            Intervention</span> (<strong><span style="color:black;">MICCAI 2023</span></strong>)
                            <br/>
                            <span class="brsmall"></span>
                            üèÖ 
                            <span style="color:#C82506;"><strong>Early accept, top ~ 14%</strong></span>
                            <br/>
                            <span style="color:black;">Also, in the 3<sup>rd</sup> Workshop on
                            Interpretable Machine Learning in Healthcare</span> (<strong><span style="color:black;">IMLH</span></strong>), <span style="color:black;">ICML 2023</span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/icml-2023.png" class="publogo float-left" width="100">
                        <div>
                            <b>Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route,
                                Interpret, Repeat</b>
                            <b>Shantanu Ghosh</b>,
                            <a href="https://gatechke.github.io/" style="color: rgb(136, 136, 136);">Ke Yu</a>,
                            <a href="https://forougha.github.io/" style="color: rgb(136, 136, 136);">
                                Forough Arabshahi</a>,
                            <a href="https://www.batman-lab.com/" style="color: rgb(136, 136, 136);">
                                Kayhan Batmanghelich
                            </a>
                            <br/>
                            <a href="https://shantanu-ai.github.io/projects/ICML-2023-MoIE/" class="mr-3">
                                Project Page
                            </a>
                            <a href="https://proceedings.mlr.press/v202/ghosh23c.html" class="mr-3">Paper</a>
                            <a href="https://arxiv.org/pdf/2307.05350.pdf" class="mr-3">arXiv</a>
                            <a href="https://openreview.net/pdf?id=m5vnLHfNy7"
                               class="mr-3">
                                Shortcut Paper
                            </a>
                            <a href="https://github.com/batmanlab/ICML-2023-Route-interpret-repeat" class="mr-3">
                                Code
                            </a>
                            <a href="https://shantanu-ai.github.io/projects/ICML-2023-MoIE/static/data/Route-Interpret-Repeat-ICML-2023-slides_v4.pdf"
                               class="mr-3">Slides</a>
                            <a href="https://shantanu-ai.github.io/projects/ICML-2023-MoIE/static/data/Route-Interpret-Repeat-ICML-2023-poster_v5.pdf"
                               class="mr-3">Poster</a>
                            <a href="https://www.youtube.com/watch?v=252zEPba8pQ&t=6s" class="mr-3">Video</a>
                            <br/>
                            <span style="color:black;">40<sup>th</sup> International Conference on Machine Learning</span> (<strong><span
                                style="color:black;">ICML 2023</span></strong>)
                            <br/>
                            <span style="color:black;">Also, in the 2<sup>nd</sup> Workshop on
                            Spurious Correlations, Invariance and Stability (<strong><span style="color:black;">SCIS</span></strong>), <span style="color:black;">ICML 2023</span></span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/MICCAI.png" class="publogo float-left" width="100">
                        <div>
                            <b>Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays</b>
                            <a href="https://gatechke.github.io/" style="color: rgb(136, 136, 136);">Ke Yu</a>,
                            <b>Shantanu Ghosh</b>,
                            <a href="https://people.cs.pitt.edu/~zhexiong/" style="color: rgb(136, 136, 136);">Zhexiong
                                Li</a>,
                            <a href="http://www.rad.pitt.edu/profile-detail.html?profileID=25/"
                               style="color: rgb(136, 136, 136);">Christopher Deible</a>,
                            <a href="https://www.batman-lab.com/" style="color: rgb(136, 136, 136);">
                                Kayhan Batmanghelich
                            </a>
                            <br/>
                            <a href="https://link.springer.com/chapter/10.1007/978-3-031-16443-9_63"
                               class="mr-3">Paper</a>
                            <a href="https://arxiv.org/pdf/2206.12704.pdf" class="mr-3">arXiv</a>
                            <a href="https://conferences.miccai.org/2022/papers/045-Paper1726.html"
                               class="mr-3">Reviews</a>
                            <a href="https://github.com/batmanlab/AGXNet" class="mr-3">
                                Code
                            </a>
                            <br/>
                            <span style="color:black;">25<sup>th</sup> International Conference on Medical Image Computing and Computer Assisted
                            Intervention</span> (<strong><span style="color:black;">MICCAI 2022</span></strong>)
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/dr-vidal.png" class="publogo float-left" width="100">
                        <div>
                            <b>DR-VIDAL-Doubly Robust Variational Information-theoretic Deep Adversarial
                                Learning for Counterfactual Prediction and Treatment Effect Estimation</b>
                            <b>Shantanu Ghosh</b>,
                            <a href="https://scholar.google.com/citations?user=7lPO8goAAAAJ&hl=en"
                               style="color: rgb(136, 136, 136);">Zheng
                                Feng</a>,
                            <a href="https://hobi.med.ufl.edu/profile/bian-jiang/" style="color: rgb(136, 136, 136);">Jiang
                                Bian</a>,
                            <a href="https://www.cise.ufl.edu/~butler/" style="color: rgb(136, 136, 136);">Kevin
                                Butler</a>,
                            <a href="https://epidemiology.phhp.ufl.edu/profile/prosperi-mattia/"
                               style="color: rgb(136, 136, 136);">Mattia Prosperi</a>
                            <br/>
                            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10148269/"
                               class="mr-3">Paper</a>
                            <a href="https://arxiv.org/pdf/2303.04201.pdf" class="mr-3">arXiv</a>
                            <a href="https://github.com/shantanu-ai/DR-VIDAL-AMIA-22" class="mr-3">
                                Code
                            </a>
                            <a href="https://github.com/shantanu-ai/DR-VIDAL-AMIA-22/blob/main/DR_VIDAL_AMIA-Supp.pdf"
                               class="mr-3">
                                Supplementary
                            </a>
                            <a href="https://docs.google.com/presentation/d/1oQA4m0oVeNmjBc6t7al1Is1evB-uVg7F/edit?usp=sharing&ouid=108402232378812733962&rtpof=true&sd=true"
                               class="mr-3">
                                Slides
                            </a>
                            <a href="https://www.youtube.com/watch?v=BB5fLYxBFV4&t=218s" class="mr-3">
                                Video
                            </a>
                            <br/>
                            <span style="color:black;">American Medical Informatics Association (<strong><span
                                style="color:black;">AMIA 2022</span></strong>) Annual Symposium</span>
                                <!-- <strong><span style="color:black;">Oral</span></strong> -->
                            <br/>
                            <span class="brsmall"></span>
                            üèÖ 
                            <span style="color:#C82506;"><strong>Long Oral Presentation</strong></span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/PSSAM-GAN.svg" class="publogo float-left" width="100">
                        <div>
                            <b>Propensity score synthetic augmentation matching using generative adversarial
                                networks (PSSAM-GAN)</b>
                            <b>Shantanu Ghosh</b>,
                            <a href="https://www.christinaboucher.com/" style="color: rgb(136, 136, 136);">Christina
                                Boucher</a>,
                            <a href=" https://hobi.med.ufl.edu/profile/bian-jiang/" style="color: rgb(136, 136,
                            136);">Jiang
                                Bian</a>,
                            <a href="https://epidemiology.phhp.ufl.edu/profile/prosperi-mattia/"
                               style="color: rgb(136, 136, 136);">Mattia Prosperi</a>
                            <br/>
                            <a href="https://www.sciencedirect.com/science/article/pii/S2666990021000197?via%3Dihub"
                               class="mr-3">Paper</a>
                            <a href="https://github.com/shantanu-ai/PSSAM-GAN" class="mr-3">
                                Code
                            </a>
                            <br/>
                            <span style="color:black;">Journal of Computer methods and programs in biomedicine update Volume 1 (2021)</span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/DPN-SA.svg" class="publogo float-left" width="100">
                        <div>
                            <b>Deep propensity network using a sparse autoencoder for estimation of treatment
                                effects</b>
                            <b>Shantanu Ghosh</b>,
                            <a href="https://scholar.google.com/citations?user=7lPO8goAAAAJ&hl=en"
                               style="color: rgb(136, 136, 136);">Zheng
                                Feng</a>,
                            <a href="https://hobi.med.ufl.edu/profile/guo-yi/" style="color: rgb(136, 136, 136);">Yi
                                Guo</a>,
                            <a href="https://hobi.med.ufl.edu/profile/bian-jiang/" style="color: rgb(136, 136, 136);">Jiang
                                Bian</a>,
                            <a href="https://epidemiology.phhp.ufl.edu/profile/prosperi-mattia/"
                               style="color: rgb(136, 136, 136);">Mattia Prosperi</a>
                            <br/>
                            <a href="https://academic.oup.com/jamia/article-abstract/28/6/1197/6139936?redirectedFrom=fulltext"
                               class="mr-3">Paper</a>
                            <a href="https://github.com/shantanu-ai/DPN-SA" class="mr-3">
                                Code
                            </a>
                            <br/>
                            <span style="color:black;">Journal of the American Medical Informatics Association (<strong><span
                                style="color:black;">JAMIA</span></strong>) Volume 28 Issue 6 (2021)</span>
                        </div>
                    </li>

                </ul>
            </div>
        </div>
    </div>

    <div class="row my-4" id="projects">
        <div class="col">
            <h4>Academic Projects</h4>
            <div>
                <ul style='list-style-type:none;margin-bottom: 0!important;padding-left: 0!important'>
                    <li class="paperitem">
                        <img alt="" src="images/arch.png" class="publogo float-left" width="100">
                        <div>
                            <b>Explaining why Lottery Ticket Hypothesis Works or Fails</b>
                            <a href="https://github.com/shantanu-ai/Explainability-with-LTH/blob/main/doc/Vlr_Proposal.pdf"
                               class="mr-3">
                                Proposal
                            </a>
                            <a href="https://github.com/shantanu-ai/Explainability-with-LTH/blob/main/doc/VLR.pdf"
                               class="mr-3">Report</a>
                            <a href="https://github.com/shantanu-ai/Explainability-with-LTH" class="mr-3">
                                Code
                            </a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a part of <a href="https://visual-learning.cs.cmu.edu/f22/index.html"><strong>CMU 16-824:
                            Visual Learning and Recognition</strong></a>, we studied the relationship between pruning
                            and explainability. We validated if the explanations generated from the pruned network using
                            Lottery ticket hypothesis (LTH) are consistent or not. Specifically we pruned a neural
                            network using LTH. Next we generated and compared the local and global explanations using
                            Grad-CAM and Concept activations respectively.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/DLCG.png" class="publogo float-left" width="100">
                        <div>
                            <b>Efficient classification by data augmentation using CGAN and InfoGAN</b>
                            <a href="https://github.com/shantanu-ai/Data-Augmentation-using-CGAN-and-InfoGAN/blob/master/Report/DL_Proj_Proposal.pdf"
                               class="mr-3">
                                Proposal
                            </a>
                            <a href="https://github.com/shantanu-ai/Data-Augmentation-using-CGAN-and-InfoGAN/blob/master/Report/DL_Final%20Report.pdf"
                               class="mr-3">Report</a>
                            <a href="https://github.com/shantanu-ai/Data-Augmentation-using-CGAN-and-InfoGAN"
                               class="mr-3">
                                Code
                            </a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a part of <strong><a
                                    href="https://coreytolerfranklin.com/course/cis6930-4930-deep-learning-for-computer-graphics-fall-2020/">CIS6930 - Deep Learning for Computer Graphics</a></strong>, we used two
                            novel variants of GAN: 1) Conditional GAN and 2) InfoGAN to augment the dataset and compare
                            the classifier‚Äôs performance using a novel dataset augmentation algorithm. Our experiments
                            showed that with less training samples from the original dataset and augmenting it using the
                            generative models, the classifier achieved similar accuracy when trained from scratch.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/Sigmoid.jpeg" class="publogo float-left" width="100">
                        <div>
                            <b>Deep Colorization</b>
                            <a href="https://github.com/shantanu-ai/Deep_Colorization/blob/master/PartII_DeepColorization.pdf"
                               class="mr-3">
                                Problem Description
                            </a>
                            <a href="https://github.com/shantanu-ai/Deep_Colorization/blob/master/Report.pdf"
                               class="mr-3">Report</a>
                            <a href="https://github.com/shantanu-ai/Deep_Colorization" class="mr-3">Code</a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a part of <strong><a
                                    href="https://coreytolerfranklin.com/course/cis6930-4930-deep-learning-for-computer-graphics-fall-2020/">CIS6930 - Deep Learning for Computer Graphics</a></strong>, a CNN was
                            created to train to color grayscale face images.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/texture.png" class="publogo float-left" width="100">
                        <div>
                            <b>Deep Multitask Texture Classifier(MTL-TCNN)</b>
                            <a href="https://github.com/shantanu-ai/MTL-TCNN3/blob/master/Report/Texture_Classification.pdf"
                               class="mr-3">Report</a>
                            <a href="https://github.com/shantanu-ai/MTL-TCNN3" class="mr-3">Code</a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a part of the independent research study in Spring 2020 (Feb - April), under<a
                                    href="http://www.wu.ece.ufl.edu/"> Dr. Dapeng Wu</a>, I developed a <strong>Deep
                            Convolutional Multitask Neural Network (MTL-TCNN)</strong> to classify textures. We used
                            an auxiliary head to detect normal images other than textures to regularize the main texture
                            detector head
                            of the network.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/TCNN3.png" class="publogo float-left" width="100">
                        <div>
                            <b>Implementation of TCNN3 paper</b>
                            <a href="https://github.com/shantanu-ai/TCNN3" class="mr-3">Code</a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a research assistant under <a
                                    href="http://www.wu.ece.ufl.edu/">Dr. Dapeng Wu</a>, I implemented TCNN3 architecture in
                            end to end manner from scratch (no pretraining) for DTD dataset, discussed in the paper
                            <a href="https://arxiv.org/pdf/1601.02919.pdf">Using filter banks in Convolutional Neural Networks for texture
                                classification</a>.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/dcn-pd.png" class="publogo float-left" width="100">
                        <div>
                            <b>Implementation of Deep Counterfactual Networks with Propensity-Dropout</b>
                            <a href="https://github.com/shantanu-ai/Deep-Counterfactual-Networks-with-Propensity-Dropout"
                               class="mr-3">Code</a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                As a research assistant of <a
                                    href="https://epidemiology.phhp.ufl.edu/research/disl/">DISL</a>, I implemented the
                            paper <a href="https://arxiv.org/pdf/1706.05966.pdf">Deep Counterfactual Networks with Propensity-Dropout</a>, which was
                            subsequently used in my other research.
                            </span>
                        </div>
                    </li>

                    <li class="paperitem">
                        <img alt="" src="images/p2p.png" class="publogo float-left" width="100">
                        <div>
                            <b>Peer to peer (p2p) network</b>
                            <a href="https://github.com/shantanu-ai/P2P-File-sharing/blob/master/Project2.pdf"
                               class="mr-3">Problem Description</a>
                            <a href="https://github.com/shantanu-ai/P2P-File-sharing"
                               class="mr-3">Code</a>
                            <a href="https://www.youtube.com/watch?v=yf0M3uXAPNc&t=16s"
                               class="mr-3">Video</a>
                            <br/>
                            <span style="color: rgb(33, 37, 41)">
                                This project was created as a part of the p2p project for <strong>Computer Networks
                            (CNT5106C)</strong> at the University of Florida for the Master's in Computer Science
                            program. A simplified peer to peer network where any number of peers can share any type of file among
                            themselves. Implemented in Java.
                            </span>
                        </div>
                    </li>

                </ul>
            </div>
        </div>
    </div>

    <div class="row my-4" id="service">
        <div class="col">
            <h4>Academic Service</h4>
            <b class="mt-3">Conference reviewer</b>
            <br/>
            <ul>
                <li>Medical Image Computing and Computer Assisted Intervention (MICCAI) 2024</li>
                <li>IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) 2024</li>
                <li>Causal Learning and Reasoning (CLeaR) 2024</li>
                <li>International Conference on Learning Representations (ICLR) 2024</li>
                <li>Association for the Advancement of Artificial Intelligence (AAAI) 2024</li>
                <li>Neural Information Processing Systems (NeurIPS) 2023</li> 
                <li>ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (BCB) 2022</li>
            </ul>

            <b class="mt-3">Journal reviewer</b>
            <br/>
            <ul>
                <li>Medical Image Analysis (MedIA)</li>
                <li>Computer Methods and Programs in Biomedicine (CMPB)</li>
                <li>Biometrical Journal</li>
            </ul>

            <b class="mt-3">Workshop reviewer</b>
            <br/>
            <ul>
                <li>Causal Representation Learning workshop (CRL), NeurIPS 2023</li>
                <li>Spurious Correlations, Invariance and Stability (SCIS), ICML 2023</li>
                <li>Interpretable Machine Learning in Healthcare (IMLH), ICML 2023</li>
            </ul>
        </div>
    </div>

    <div class="row my-4" id="teaching">
        <div class="col-xl-7">
            <h4>Teaching</h4>
            <h6 class="text-muted mt-3">Courses</h6>
            <div class="class-listing">
                <span><b>Introduction to Software Engineering (EC 327) - Fall 2023</b><br>
                Teaching Assistant</span><span class="text-muted">Boston University</span>
            </div>
        </div>
    </div>
    <h4>Talks</h4>
    <div class="row my-4" id="talks">
        <div class="col-lg-6 mb-4">
            <div class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/cg6vFS8C0mI?si=jRO9vPgTz5iBjQbg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                <p>Oral Presentation @ AMIA 2022 Annual Symposium [Slides]</p>
            </div>
        </div>
        <div class="col-lg-6 mb-4">
            <div class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/sXzh5cIeJD8?si=vUzLnCFfJcjJoepu" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                <p>Fall 2023, ISP AI Forum @ University of Pittsburgh [Slides]</p>
            </div>
        </div>
        <!-- Repeat the above pattern for more videos -->
    </div>


    <h4>Tutorials</h4>
    <div class="row my-4" id="tutorials">
        <div class="col-lg-6 mb-4">
            <div class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/0lDNgFPyS_c?si=Ud9T_j2n5sC_xH2P" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                <p class="video-title">Tutorial on Variational Autoencoder (VAE)</p>
            </div>
        </div>
        <div class="col-lg-6 mb-4">
            <div class="video-container">
                <p class="video-title">Tutorial on Pearl's Do Calculus of causality</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/RCclqTLSsw0?si=AYS6nW9SxFeSvFyL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                
            </div>
        </div>
        <!-- Repeat the above pattern for more videos -->
    </div>


    <div class="row my-4" id="past">
        <div class="col">
            <h4>As a software engineer</h4>
            In my past life, I spent 6+ years in software service/product development as
            a full stack software engineer across Lexmark International India Pvt Ltd
            and Cognizant Technology Solutions India Pvt Ltd,
            using Angular/Angular.js, C#/.Net, WCF web services, Node.js, Oracle and MS SQL Server. For Cognizant, I
            used to build WCF webservices using contract-first approach. For Lexmark, I was a part of the development
            team which created <a
                href="https://infoserve.lexmark.com/ids/idv/video.aspx?category=All&productCode=PUBLISHING_PLATFORM_FOR_RETAIL&topic=v54408848&vId=PPR%2fLexmark-PPR-ISP-Overview-video&ar=16%3a9&l1=1&l2=1&sh=764&sw=1440&loc=en_US">this</a>.
        </div>
    </div>

    <div class="row my-4" id="community">
        <div class="col">
            <h4>Community service</h4>
            I have been an active member of <a
                href="https://www.cognizant.com/us/en/about-cognizant/esg/outreach-program">Cognizant Kolkata Outreach
            council</a> (NGO related engagement for development of underprivileged children in Kolkata). Refer to the <a
                href="https://drive.google.com/file/d/1xE-ihuTTDndMZKqnzNNkJ0UyCOBYac_3/view">link</a> for the
            certificate of recognition. Refer <a
                href="https://drive.google.com/drive/folders/1P_7Tr5l4cxGDv6b1qaEZq1lyYhPprj1D">here </a> for pictures
            clicked by me in one of such events in 2014.
        </div>
    </div>
    <div class="row my-4" id="misc">
        <div class="col">
            <h4>Miscellaneous</h4>
            I am originally from <a href="https://en.wikipedia.org/wiki/Kolkata">Kolkata</a>, once the capital of India.
            I have lived in Gainesville (FL), Pittsburgh (PA) and Boston (MA).
        </div>
    </div>
    <div class="row my-4" id="interviews">
    <div class="col">
        <h4>Interviews</h4>
        <ul>
            <li>
                Podcast hosted by <a href="https://kdmsit.github.io/">Kishlay Das</a> to provide my insights for the admission and research in the US for MS/PhD aspirants.
                <div class="video-container" style="max-width: 860px; margin: 0 auto;">
                    <iframe width="860" height="315" src="https://www.youtube.com/embed/rl4HKbNmXTQ?si=yOBbYgLzS3M8zhJL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </li>
        </ul>
    </div>
</div>

</main>

<footer class=" bg-secondary text-white">
    <div class=" container py-4" id="contact" style="margin: 0 auto">
        <div class="row">
            <div class="col">
                <h4>CONTACT</h4>
                <span class="text-white-50">Email: </span>shawn24 [at] bu [dot] edu<br>
                <span class="text-white-50">Office: </span>

                <span style="">407-07 Photonics Building</span> <br/>
                <font size="-1">¬© Shantanu Ghosh 2023. Feel free to use <a
                        href="https://github.com/shantanu-ai/shantanu-ai.github.io">my template</a> adapted
                    from <a
                            href="https://purvaten.github.io/"> Purva Tendulkar</a>.</font>
                
            </div>
            <div class="col">
                <h4>HELPFUL LINKS</h4>
                <a href="https://www.batman-lab.com/" class="text-light">Batman Lab @ BU</a><br/>
                <a href="https://epidemiology.phhp.ufl.edu/disl/" class="text-light">DISL Lab @ UF</a><br/>
                <!-- <a href="files/Shantanu_Ghosh_Resume.pdf" class="text-light">CV</a> <br /> -->
                <a href="https://www.bu.edu/eng/academics/departments-and-divisions/electrical-and-computer-engineering/" class="text-light">ECE Department @ BU</a><br/>
                <a href="https://www.isp.pitt.edu/" class="text-light">ISP @ Pitt</a><br/>
                <a href="https://www.cise.ufl.edu/" class="text-light">CISE @ UF</a><br/>
            </div>
        </div>
    </div>
</footer>
</body>
</html>
